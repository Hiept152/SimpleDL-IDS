{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ryxC2UdJzVwT"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the dataset (replace 'dataset.csv' with your actual dataset file)\n",
        "#data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Assuming the dataset has features X and labels y\n",
        "#X = data.drop('label', axis=1)\n",
        "#y = data['label']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building the RNN model\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(units=64, input_shape=(X_train.shape[1], 1)))\n",
        "#model.add(Dense(units=32, activation='relu'))\n",
        "#model.add(Dense(units=len(y.unique()), activation='softmax'))  # Adjust units for multiclass classification\n",
        "#model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the RNN model\n",
        "#model.fit(np.expand_dims(X_train, axis=-1), y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluating the RNN model\n",
        "#_, accuracy = model.evaluate(np.expand_dims(X_test, axis=-1), y_test)\n",
        "#print(f\"RNN Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Integrating Random Forest Algorithm as a multiclassifier\n",
        "#rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "#rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Testing the integrated model\n",
        "#rf_predictions = rf_classifier.predict(X_test)\n",
        "#rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "#print(f\"Random Forest Model Accuracy: {rf_accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (replace 'dataset.csv' with your actual dataset file)\n",
        "data = pd.read_csv('Monday-WorkingHoursCVE.pcap_ISCX.csv')\n"
      ],
      "metadata": {
        "id": "Dok6IG-M5cpk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "# Assuming the dataset has features X and labels y\n",
        "X = data.drop(' Label', axis=1)\n",
        "y = data[' Label']\n",
        "\n",
        "# Convert categorical labels to numerical format\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n"
      ],
      "metadata": {
        "id": "ZSXnqpCX5h4e"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "VLRp5rmd5ohr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the RNN model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(y.unique()), activation='softmax'))  # Adjust units for multiclass classification\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Gdbw0OHI5sGs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the RNN model\n",
        "model.fit(np.expand_dims(X_train, axis=-1), y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "eL550bng5vbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16b6a90-1abd-407d-9e6e-e99c88b5dba5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10599/10599 [==============================] - 408s 38ms/step - loss: nan - accuracy: 0.9888 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "10599/10599 [==============================] - 414s 39ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "10599/10599 [==============================] - 406s 38ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "10599/10599 [==============================] - 409s 39ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "10599/10599 [==============================] - 417s 39ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "10599/10599 [==============================] - 411s 39ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10599/10599 [==============================] - 412s 39ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10599/10599 [==============================] - 422s 40ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10599/10599 [==============================] - 419s 40ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10599/10599 [==============================] - 423s 40ms/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a51c2be9a50>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the RNN model\n",
        "_, accuracy = model.evaluate(np.expand_dims(X_test, axis=-1), y_test)\n",
        "print(f\"RNN Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "hw99e0pA5yjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dbb668-bc1c-4040-8ba6-0fb168adf802"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3312/3312 [==============================] - 41s 12ms/step - loss: nan - accuracy: 1.0000\n",
            "RNN Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert X_train to a DataFrame\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = X_train_df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A_33gEhfR64",
        "outputId": "40574786-ecec-4dea-dcd3-3024bf91f747"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Destination Port              0\n",
            " Flow Duration                 0\n",
            " Total Fwd Packets             0\n",
            " Total Backward Packets        0\n",
            "Total Length of Fwd Packets    0\n",
            "                              ..\n",
            " Active Min                    0\n",
            "Idle Mean                      0\n",
            " Idle Std                      0\n",
            " Idle Max                      0\n",
            " Idle Min                      0\n",
            "Length: 78, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrating Random Forest Algorithm as a multiclassifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "6BVBWWZG503G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "fbb10cdd-4ead-4c54-aae5-9e76675cc2ec"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-812fba945145>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Integrating Random Forest Algorithm as a multiclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the integrated model\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f\"Random Forest Model Accuracy: {rf_accuracy}\")\n"
      ],
      "metadata": {
        "id": "EQVrRJlj52gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}